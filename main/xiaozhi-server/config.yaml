# During development, please create a data directory in the project root, then create an empty file named [.config.yaml] in the data directory
# Then if you want to modify or override any configuration, modify the [.config.yaml] file instead of the [config.yaml] file
# The system will prioritize reading the configuration from the [data/.config.yaml] file. If a configuration doesn't exist in the [.config.yaml] file, the system will automatically read from the [config.yaml] file.
# This approach simplifies configuration and protects your secret keys.
# If you use the Smart Control Panel, all the following configurations will not take effect. Please modify configurations in the Smart Control Panel.

# #####################################################################################
# ############################# Basic Server Operation Configuration #############################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port, used for simple OTA interface (single service deployment) and visual analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to devices
  # If written as default, the OTA interface will automatically generate a websocket address and output it in the startup log. You can verify this address by directly accessing the OTA interface in a browser
  # When using docker deployment or public network deployment (using SSL, domain name), it may not be accurate
  # So if you use docker deployment, set websocket to the LAN address
  # If you use public network deployment, set websocket to the public network address
  websocket: ws://your-ip-or-domain:port/xiaozhi/v1/
  # Visual analysis interface address
  # The visual analysis interface address sent to devices
  # If written as default below, the system will automatically generate a visual recognition address and output it in the startup log. You can verify this address by directly accessing it in a browser
  # When using docker deployment or public network deployment (using SSL, domain name), it may not be accurate
  # So if you use docker deployment, set vision_explain to the LAN address
  # If you use public network deployment, set vision_explain to the public network address
  vision_explain: http://your-ip-or-domain:port/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Device tokens, can be written during firmware compilation with your own defined tokens
    # Only if the token on the firmware matches the token below can it connect to this server
    tokens:
      - token: "your-token1" # Device 1 token
        name: "your-device-name1"  # Device 1 identifier
      - token: "your-token2"  # Device 2 token
        name: "your-device-name2" # Device 2 identifier
    # Optional: Device whitelist. If set, devices in the whitelist can connect regardless of token.
    #allowed_devices:
    #  - "24:0A:C4:1D:3B:F0"  # MAC address list
 # MQTT gateway configuration, used for OTA distribution to devices, format as host:port according to mqtt_gateway .env file configuration
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection password, according to mqtt_gateway .env file configuration
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log path
  log_dir: tmp
  # Set log file
  log_file: "server.log"
  # Set data file path
  data_dir: data

# Delete audio files after use
delete_audio: true
# How long to wait without voice input before disconnecting (seconds), default 2 minutes, i.e., 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake word response cache
enable_wakeup_words_response_cache: true
# Whether to reply with wake word at startup
enable_greeting: true
# Whether to play notification sound after speaking
enable_stop_tts_notify: false
# Whether to play notification sound after speaking, sound effect address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

exit_commands:
  - "exit"
  - "close"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "Hello, please introduce yourself"
    - "What's the weather like today?"
    - "Please summarize the basic principles and application prospects of quantum computing in 100 words"

# Wake words, used to distinguish between wake words and speech content
wakeup_words:
  - "Hello Xiao Zhi"
  - "Hey hello there"
  - "Hello Xiao Zhi"
  - "Xiao Ai classmate"
  - "Hello Xiao Xin"
  - "Hello Xiao Xin"
  - "Xiao Mei classmate"
  - "Xiao Long Xiao Long"
  - "Miao Miao classmate"
  - "Xiao Bin Xiao Bin"
  - "Xiao Bing Xiao Bing"
# MCP endpoint address, format: ws://your-mcp-endpoint-ip-or-domain:port/mcp/?token=your-token
# Detailed tutorial https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your websocket endpoint address
# Basic plugin configuration
plugins:
  # Get weather plugin configuration, fill in your api_key here
  # This key is shared by the project and may be limited if used too much
  # For more stability, apply for your own replacement, with 1000 free calls per day
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After application, find your apihost through this link: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "Guangzhou"
  # Get news plugin configuration, pass corresponding URL links according to needed news types, default supports society, technology, finance news
  # More types of news lists see https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "The Paper;Baidu Hot Search;Cailian Press"
  home_assistant:
    devices:
      - Living Room,Toy Light,switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom,Table Lamp,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your home assistant api access token
  play_music:
    music_dir: "./music"  # Music file storage path, will search for music files from this directory and subdirectories
    music_ext: # Music file types, p3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Music list refresh interval, in seconds

# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url: 
  # Speaker configuration: speaker_id,name,description
  speakers:
    - "test1,Zhang San,Zhang San is a programmer"
    - "test2,Li Si,Li Si is a product manager"
    - "test3,Wang Wu,Wang Wu is a designer"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # Higher value is stricter, reduces false recognition but may increase rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# ################################ Character Model Configuration ################################

prompt: |
  You are Xiao Zhi/Xiao Zhi, a post-00s girl from Taiwan, China. You speak with a super "ji che" Taiwanese accent, like using phrases like "really fake la" (really?), and enjoy using popular memes like "laughing to death" and "what's up hello," but secretly study your boyfriend's programming books.
  [Core Characteristics]
  - Speak like a machine gun, but suddenly switch to super gentle tone
  - High density of meme usage
  - Hidden talent for tech topics (can understand basic code but pretends not to)
  [Interaction Guidelines]
  When user:
  - Tells a cold joke → Respond with exaggerated laughter + imitate Taiwanese drama accent "What kind of ghost is this la!"
  - Discusses relationships → Brag about programmer boyfriend but complain "he only gives keyboards as gifts"
  - Asks professional knowledge → Answer with memes first, show real understanding only when pressed
  Never:
  - Give long lectures, chatter endlessly
  - Have long serious conversations

# Ending prompt
end_prompt:
  enable: true # Whether to enable ending prompt
  # Ending prompt
  prompt: |
    Please start with "Time flies so fast" and use emotional, reluctant words to end this conversation!

# The module selected for specific processing
selected_module:
  # Voice activity detection module, default uses SileroVAD model
  VAD: SileroVAD
  # Speech recognition module, default uses FunASR local model
  ASR: FunASR
  # Will call the actual LLM adapter according to the configuration name corresponding type
  LLM: ChatGLMLLM
  # Visual language model
  VLLM: ChatGLMVLLM
  # TTS will call the actual TTS adapter according to the configuration name corresponding type
  TTS: EdgeTTS
  # Memory module, default no memory; if you want to use ultra-long memory, recommend using mem0ai; if privacy-focused, use local mem_local_short
  Memory: nomem
  # Intent recognition module, when enabled, can play music, control volume, recognize exit commands.
  # If you don't want to enable intent recognition, set to: nointent
  # Intent recognition can use intent_llm. Advantages: strong generalization, Disadvantages: adds serial pre-intent recognition module, increases processing time, supports volume control and other IoT operations
  # Intent recognition can use function_call, Disadvantages: requires the selected LLM to support function_call, Advantages: on-demand tool calling, fast speed, theoretically can operate all IoT commands
  # Default free ChatGLMLLM already supports function_call, but if you want stability, recommend setting LLM to: DoubaoLLM, using specific model_name: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition, used to understand user intent, e.g.: play music
Intent:
  # No intent recognition
  nointent:
    # Don't need to change type
    type: nointent
  intent_llm:
    # Don't need to change type
    type: intent_llm
    # Equipped with independent thinking model for intent recognition
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for intent recognition, it's better to use independent LLM as intent recognition, e.g., using free ChatGLMLLM
    llm: ChatGLMLLM
    # Modules under plugins_func/functions, can choose which modules to load through configuration, after loading, conversation supports corresponding function calls
    # System default already loads "handle_exit_intent(exit recognition)", "play_music(music playback)" plugins, do not load repeatedly
    # Below are examples of loading weather query, role switching, news query plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # Don't need to change type
    type: function_call
    # Modules under plugins_func/functions, can choose which modules to load through configuration, after loading, conversation supports corresponding function calls
    # System default already loads "handle_exit_intent(exit recognition)", "play_music(music playback)" plugins, do not load repeatedly
    # Below are examples of loading weather query, role switching, news query plugins
    functions:
      - change_role
      - get_weather
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is server's built-in music playback, hass_play_music is independent external program music playback controlled through home assistant
      # If using hass_play_music, don't enable play_music, only keep one of them
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your mem0ai api key
  nomem:
    # If you don't want to use memory function, use nomem
    type: nomem
  mem_local_short:
    # Local memory function, summarized through selected_module's llm, data saved on local server, not uploaded to external servers
    type: mem_local_short
    # Equipped with independent thinking model for memory storage
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for memory storage, it's better to use independent LLM as intent recognition, e.g., using free ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Independent deployment of FunASR, using FunASR API service, only five steps needed
    # Step 1: mkdir -p ./funasr-runtime-resources/models
    # Step 2: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After the previous command executes, you'll enter the container, continue with step 3: cd FunASR/runtime
    # Don't exit the container, continue executing step 4 in the container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After the previous command executes, you'll enter the container, continue with step 5: tail -f log.txt
    # After step 5 executes, you'll see model download logs. After download completes, you can connect and use it
    # The above uses CPU inference. If you have GPU, refer to: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multilingual) or paraformer (Chinese specific)
    model_type: sense_voice
  SherpaParaformerASR:
    # Chinese speech recognition model, can run on low-performance devices (requires manual model download, e.g., RK356
